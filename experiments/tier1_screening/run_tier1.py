#!/usr/bin/env python3
"""
Tier 1: Quick Screening
=======================

Fast elimination of clearly inferior options before heavy compute.
Uses 20% of data, 20 epochs, single seed/fold.

Purpose: Reduce 8 arch Ã— 6 loss = 48 combinations down to top-3 each.

GATE: At least one neural method must beat classical floor + 0.05

Usage:
    python experiments/tier1_screening/run_tier1.py
    python experiments/tier1_screening/run_tier1.py --dry-run
"""

from __future__ import annotations

import argparse
import gc
import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# =============================================================================
# CRITICAL: Fix Python path for imports
# =============================================================================
PROJECT_ROOT = Path(__file__).parent.parent.parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, Subset
from torch.amp import autocast, GradScaler

from experiments.common.config_registry import (
    ScreeningResult,
    Tier1Result,
    GateFailure,
    get_registry,
)
from experiments.study1_architecture.architectures import create_architecture, ARCHITECTURE_REGISTRY
from experiments.study3_loss.losses import create_loss


# =============================================================================
# Configuration
# =============================================================================

ARTIFACTS_DIR = PROJECT_ROOT / "artifacts" / "tier1_screening"
ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

# 8 architectures (covers all paradigms)
ARCHITECTURES = list(ARCHITECTURE_REGISTRY.keys())  # linear, cnn, wavenet, fnet, vit, performer, mamba

# Add unet if available
try:
    from models import UNet1DConditioned
    ARCHITECTURES = ["unet"] + ARCHITECTURES
except ImportError:
    pass

# 6 loss categories (representatives from each family)
LOSS_CATEGORIES = {
    "huber": "huber",           # Point-wise robust
    "spectral": "multi_scale_spectral",  # Spectral fidelity
    "ccc": "concordance",       # Correlation-based
    "ssim": "ssim",             # Structure-preserving
    "gaussian_nll": "gaussian_nll",  # Probabilistic
    "combined": "combined",     # Time + frequency
}

# Training hyperparameters (full training, not screening)
SCREEN_DATA_FRACTION = 1.0   # 100% data - we have 8x A100s
SCREEN_EPOCHS = 50           # Full training
SCREEN_BATCH_SIZE = 16       # A100 can handle this
SCREEN_LR = 1e-3


# =============================================================================
# Model Creation Helpers
# =============================================================================

def create_model(arch_name: str, in_channels: int, out_channels: int, device: torch.device) -> nn.Module:
    """Create model by architecture name.

    Args:
        arch_name: Architecture name
        in_channels: Input channels
        out_channels: Output channels
        device: Device to place model on

    Returns:
        Model instance

    Variant mapping:
        - linear: simple
        - cnn: basic (NOT standard!)
        - wavenet: standard
        - fnet: standard
        - vit: standard
        - performer: standard
        - mamba: standard
    """
    # Variant mapping - each arch has different default variants
    VARIANT_MAP = {
        "linear": "simple",
        "cnn": "basic",  # CNN uses "basic", not "standard"
        "wavenet": "standard",
        "fnet": "standard",
        "vit": "standard",
        "performer": "standard",
        "mamba": "standard",
    }

    if arch_name == "unet":
        from models import UNet1DConditioned
        model = UNet1DConditioned(
            in_channels=in_channels,
            out_channels=out_channels,
            base_channels=32,  # Smaller for screening
        )
    elif arch_name in VARIANT_MAP:
        variant = VARIANT_MAP[arch_name]
        model = create_architecture(arch_name, variant=variant, in_channels=in_channels, out_channels=out_channels)
    else:
        raise ValueError(f"Unknown architecture: {arch_name}")

    return model.to(device)


# =============================================================================
# Quick Training
# =============================================================================

def quick_train(
    model: nn.Module,
    criterion: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    device: torch.device,
    n_epochs: int = 20,
    lr: float = 1e-3,
    use_amp: bool = True,
) -> Dict[str, float]:
    """Quick training for screening with AMP for memory efficiency.

    Args:
        model: Model to train
        criterion: Loss function
        train_loader: Training data
        val_loader: Validation data
        device: Device
        n_epochs: Number of epochs
        lr: Learning rate
        use_amp: Use automatic mixed precision (reduces memory ~50%)

    Returns:
        Dictionary with r2, mae, and training time
    """
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)

    # AMP scaler for mixed precision training
    scaler = GradScaler('cuda') if use_amp and device.type == 'cuda' else None

    start_time = time.time()

    for epoch in range(n_epochs):
        # Train
        model.train()
        for batch in train_loader:
            if len(batch) == 3:
                x, y, _ = batch
            else:
                x, y = batch
            x, y = x.to(device), y.to(device)

            optimizer.zero_grad()

            # Mixed precision forward pass
            if scaler is not None:
                with autocast('cuda'):
                    y_pred = model(x)
                    try:
                        loss = criterion(y_pred, y)
                    except Exception:
                        loss = nn.functional.l1_loss(y_pred, y)

                if torch.isnan(loss) or torch.isinf(loss):
                    continue

                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                y_pred = model(x)
                try:
                    loss = criterion(y_pred, y)
                except Exception:
                    loss = nn.functional.l1_loss(y_pred, y)

                if torch.isnan(loss) or torch.isinf(loss):
                    continue

                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()

        scheduler.step()

    training_time = time.time() - start_time

    # Evaluate (use AMP for inference too to save memory)
    model.eval()
    all_preds, all_targets = [], []

    with torch.no_grad():
        for batch in val_loader:
            if len(batch) == 3:
                x, y, _ = batch
            else:
                x, y = batch
            x, y = x.to(device), y.to(device)

            if scaler is not None:
                with autocast('cuda'):
                    y_pred = model(x)
            else:
                y_pred = model(x)

            all_preds.append(y_pred.float().cpu())
            all_targets.append(y.cpu())

    preds = torch.cat(all_preds, dim=0)
    targets = torch.cat(all_targets, dim=0)

    # RÂ²
    ss_res = ((targets - preds) ** 2).sum()
    ss_tot = ((targets - targets.mean()) ** 2).sum()
    r2 = (1 - ss_res / (ss_tot + 1e-8)).item()

    # MAE
    mae = (targets - preds).abs().mean().item()

    return {
        "r2": r2,
        "mae": mae,
        "training_time": training_time,
    }


# =============================================================================
# Checkpoint Support
# =============================================================================

def _load_checkpoint() -> Dict[str, ScreeningResult]:
    """Load checkpoint for intra-tier resume."""
    checkpoint_file = ARTIFACTS_DIR / "tier1_checkpoint.json"
    if not checkpoint_file.exists():
        return {}

    with open(checkpoint_file) as f:
        data = json.load(f)

    results = {}
    for key, r in data.items():
        results[key] = ScreeningResult(
            architecture=r["architecture"],
            loss=r["loss"],
            r2=r["r2"],
            mae=r["mae"],
            training_time=r["training_time"],
        )
    return results


def _save_checkpoint(results: Dict[str, ScreeningResult]) -> None:
    """Save checkpoint for intra-tier resume."""
    checkpoint_file = ARTIFACTS_DIR / "tier1_checkpoint.json"
    data = {}
    for key, r in results.items():
        data[key] = {
            "architecture": r.architecture,
            "loss": r.loss,
            "r2": r.r2,
            "mae": r.mae,
            "training_time": r.training_time,
        }
    with open(checkpoint_file, "w") as f:
        json.dump(data, f, indent=2)


def _clear_checkpoint() -> None:
    """Clear checkpoint after successful tier completion."""
    checkpoint_file = ARTIFACTS_DIR / "tier1_checkpoint.json"
    if checkpoint_file.exists():
        checkpoint_file.unlink()


# =============================================================================
# Multi-GPU Training Worker
# =============================================================================

def _train_combination_on_gpu(
    arch_name: str,
    loss_category: str,
    loss_name: str,
    gpu_id: int,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_val: np.ndarray,
    y_val: np.ndarray,
    in_channels: int,
    out_channels: int,
    n_epochs: int,
    batch_size: int,
    lr: float,
) -> Dict[str, Any]:
    """Train a single arch+loss combination on a specific GPU.

    This runs in a separate process with isolated CUDA context.
    Includes OOM retry with smaller batch size.
    """
    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)

    # Limit threads to avoid contention
    os.environ["OMP_NUM_THREADS"] = "4"
    os.environ["MKL_NUM_THREADS"] = "4"

    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset
    from torch.amp import autocast, GradScaler

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # OOM retry: try progressively smaller batch sizes
    batch_sizes_to_try = [batch_size, batch_size // 2, batch_size // 4, 2]
    last_error = None

    for current_batch_size in batch_sizes_to_try:
        if current_batch_size < 1:
            continue

        # Clear GPU memory before each attempt
        torch.cuda.empty_cache()

        try:
            # Convert numpy to tensors
            X_train_t = torch.from_numpy(X_train).float()
            y_train_t = torch.from_numpy(y_train).float()
            X_val_t = torch.from_numpy(X_val).float()
            y_val_t = torch.from_numpy(y_val).float()

            # Create dataloaders with current batch size
            train_dataset = TensorDataset(X_train_t, y_train_t)
            val_dataset = TensorDataset(X_val_t, y_val_t)
            train_loader = DataLoader(train_dataset, batch_size=current_batch_size, shuffle=True, drop_last=True)
            val_loader = DataLoader(val_dataset, batch_size=current_batch_size, shuffle=False)

            # Create model (import here for fresh module state)
            from experiments.study1_architecture.architectures import create_architecture, ARCHITECTURE_REGISTRY
            from experiments.study3_loss.losses import create_loss

            VARIANT_MAP = {
                "linear": "simple",
                "cnn": "basic",
                "wavenet": "standard",
                "fnet": "standard",
                "vit": "standard",
                "performer": "standard",
                "mamba": "standard",
            }

            if arch_name == "unet":
                from models import UNet1DConditioned
                model = UNet1DConditioned(in_channels=in_channels, out_channels=out_channels, base_channels=32)
            else:
                variant = VARIANT_MAP.get(arch_name, "standard")
                model = create_architecture(arch_name, variant=variant, in_channels=in_channels, out_channels=out_channels)

            model = model.to(device)

            # Create loss
            criterion = create_loss(loss_name)
            if hasattr(criterion, 'to'):
                criterion = criterion.to(device)

            # Training with AMP
            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)
            scaler = GradScaler('cuda')

            import time
            start_time = time.time()

            for epoch in range(n_epochs):
                model.train()
                for batch in train_loader:
                    x, y_batch = batch
                    x, y_batch = x.to(device), y_batch.to(device)

                    optimizer.zero_grad()
                    with autocast('cuda'):
                        y_pred = model(x)
                        try:
                            loss = criterion(y_pred, y_batch)
                        except Exception:
                            loss = nn.functional.l1_loss(y_pred, y_batch)

                    if torch.isnan(loss) or torch.isinf(loss):
                        continue

                    scaler.scale(loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()

                scheduler.step()

            training_time = time.time() - start_time

            # Evaluate
            model.eval()
            all_preds, all_targets = [], []
            with torch.no_grad():
                for batch in val_loader:
                    x, y_batch = batch
                    x, y_batch = x.to(device), y_batch.to(device)
                    with autocast('cuda'):
                        y_pred = model(x)
                    all_preds.append(y_pred.float().cpu())
                    all_targets.append(y_batch.cpu())

            preds = torch.cat(all_preds, dim=0)
            targets = torch.cat(all_targets, dim=0)

            ss_res = ((targets - preds) ** 2).sum()
            ss_tot = ((targets - targets.mean()) ** 2).sum()
            r2 = (1 - ss_res / (ss_tot + 1e-8)).item()
            mae = (targets - preds).abs().mean().item()

            # Cleanup GPU memory
            del model, optimizer, scaler
            torch.cuda.empty_cache()

            return {
                "arch": arch_name,
                "loss": loss_category,
                "r2": r2,
                "mae": mae,
                "training_time": training_time,
                "gpu_id": gpu_id,
                "batch_size_used": current_batch_size,
                "success": True,
            }

        except RuntimeError as e:
            # Check if it's an OOM error
            if "out of memory" in str(e).lower():
                last_error = e
                # Cleanup and try smaller batch
                torch.cuda.empty_cache()
                continue
            else:
                # Non-OOM error, return failure
                import traceback
                torch.cuda.empty_cache()
                return {
                    "arch": arch_name,
                    "loss": loss_category,
                    "r2": float("-inf"),
                    "mae": float("inf"),
                    "training_time": 0,
                    "gpu_id": gpu_id,
                    "success": False,
                    "error": str(e),
                    "traceback": traceback.format_exc(),
                }

        except Exception as e:
            import traceback
            torch.cuda.empty_cache()
            return {
                "arch": arch_name,
                "loss": loss_category,
                "r2": float("-inf"),
                "mae": float("inf"),
                "training_time": 0,
                "gpu_id": gpu_id,
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc(),
            }

    # All batch sizes failed with OOM
    return {
        "arch": arch_name,
        "loss": loss_category,
        "r2": float("-inf"),
        "mae": float("inf"),
        "training_time": 0,
        "gpu_id": gpu_id,
        "success": False,
        "error": f"OOM even with batch_size=2: {last_error}",
    }


# =============================================================================
# Screening Matrix
# =============================================================================

def run_screening_matrix(
    X: torch.Tensor,
    y: torch.Tensor,
    architectures: List[str],
    losses: Dict[str, str],
    device: torch.device,
    data_fraction: float = 1.0,
    n_epochs: int = 50,
    batch_size: int = 16,
    seed: int = 42,
    resume: bool = True,
) -> List[ScreeningResult]:
    """Run full archÃ—loss matrix with MULTI-GPU parallel execution.

    Uses 8 GPUs to train combinations in parallel (1 combo per GPU).

    Args:
        X: Input data [N, C, T]
        y: Target data [N, C, T]
        architectures: List of architecture names
        losses: Dict of loss_category -> loss_name
        device: Device (ignored - uses all available GPUs)
        data_fraction: Fraction of data to use (1.0 = all)
        n_epochs: Training epochs
        batch_size: Batch size
        seed: Random seed
        resume: Whether to resume from checkpoint

    Returns:
        List of ScreeningResult for each combination
    """
    from concurrent.futures import ProcessPoolExecutor, as_completed
    import multiprocessing as mp

    torch.manual_seed(seed)
    np.random.seed(seed)

    # Load checkpoint for intra-tier resume
    completed_results = {}
    if resume:
        completed_results = _load_checkpoint()
        if completed_results:
            print(f"\nðŸ”„ Resuming from checkpoint: {len(completed_results)} combinations already done")

    n_samples = X.shape[0]
    in_channels = X.shape[1]
    out_channels = y.shape[1]

    # Subsample data (1.0 = use all)
    n_screen = int(n_samples * data_fraction)
    indices = np.random.permutation(n_samples)[:n_screen]

    # Split into train/val (80/20)
    n_train = int(n_screen * 0.8)
    train_idx = indices[:n_train]
    val_idx = indices[n_train:]

    # Convert to numpy for multiprocessing (tensors can't be pickled across spawn)
    X_np = X.numpy() if isinstance(X, torch.Tensor) else X
    y_np = y.numpy() if isinstance(y, torch.Tensor) else y

    X_train = X_np[train_idx]
    y_train = y_np[train_idx]
    X_val = X_np[val_idx]
    y_val = y_np[val_idx]

    print(f"  Training data: {X_train.shape}, Validation data: {X_val.shape}")

    # Get available GPUs
    n_gpus = torch.cuda.device_count()
    if n_gpus == 0:
        raise RuntimeError("No GPUs available! Tier 1 requires GPU.")

    print(f"  GPUs available: {n_gpus}")

    # Build list of combinations to run
    all_combos = []
    for arch_name in architectures:
        for loss_category, loss_name in losses.items():
            combo_key = f"{arch_name}_{loss_category}"
            if combo_key not in completed_results:
                all_combos.append((arch_name, loss_category, loss_name))

    total_combos = len(architectures) * len(losses)
    combos_to_run = len(all_combos)

    print(f"\n  Running {combos_to_run} combinations on {n_gpus} GPUs ({total_combos - combos_to_run} already done)")

    # Add completed results first
    results = []
    for arch_name in architectures:
        for loss_category, _ in losses.items():
            combo_key = f"{arch_name}_{loss_category}"
            if combo_key in completed_results:
                results.append(completed_results[combo_key])

    if combos_to_run == 0:
        print("  All combinations already completed!")
        return results

    # Run combinations in parallel across GPUs
    ctx = mp.get_context('spawn')
    n_workers = min(n_gpus, combos_to_run)

    # Import tqdm for progress bar
    try:
        from tqdm import tqdm
        HAS_TQDM = True
    except ImportError:
        HAS_TQDM = False

    with ProcessPoolExecutor(max_workers=n_workers, mp_context=ctx) as executor:
        future_to_combo = {}

        for i, (arch_name, loss_category, loss_name) in enumerate(all_combos):
            gpu_id = i % n_gpus
            future = executor.submit(
                _train_combination_on_gpu,
                arch_name, loss_category, loss_name, gpu_id,
                X_train, y_train, X_val, y_val,
                in_channels, out_channels,
                n_epochs, batch_size, SCREEN_LR,
            )
            future_to_combo[future] = (arch_name, loss_category)

        # Process results as they complete with tqdm progress bar
        if HAS_TQDM:
            pbar = tqdm(
                total=combos_to_run,
                desc="  Training",
                unit="combo",
                ncols=100,
                bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
            )

        for future in as_completed(future_to_combo):
            arch_name, loss_category = future_to_combo[future]
            combo_key = f"{arch_name}_{loss_category}"

            try:
                result_dict = future.result()
                r2 = result_dict["r2"]
                mae = result_dict["mae"]
                training_time = result_dict["training_time"]
                gpu_id = result_dict["gpu_id"]

                if result_dict["success"]:
                    msg = f"{arch_name}+{loss_category} @GPU{gpu_id}: RÂ²={r2:.4f} ({training_time:.1f}s)"
                    if HAS_TQDM:
                        pbar.set_postfix_str(msg)
                        tqdm.write(f"  âœ“ {msg}")
                    else:
                        print(f"  âœ“ {msg}")
                else:
                    msg = f"{arch_name}+{loss_category} @GPU{gpu_id}: FAILED - {result_dict.get('error', 'Unknown')[:50]}"
                    if HAS_TQDM:
                        tqdm.write(f"  âœ— {msg}")
                    else:
                        print(f"  âœ— {msg}")

                result = ScreeningResult(
                    architecture=arch_name,
                    loss=loss_category,
                    r2=r2,
                    mae=mae,
                    training_time=training_time,
                )

                # Save checkpoint
                completed_results[combo_key] = result
                _save_checkpoint(completed_results)

            except Exception as e:
                msg = f"{arch_name}+{loss_category}: FAILED - {e}"
                if HAS_TQDM:
                    tqdm.write(f"  âœ— {msg}")
                else:
                    print(f"  âœ— {msg}")
                result = ScreeningResult(
                    architecture=arch_name,
                    loss=loss_category,
                    r2=float("-inf"),
                    mae=float("inf"),
                    training_time=0,
                )
                completed_results[combo_key] = result
                _save_checkpoint(completed_results)

            if HAS_TQDM:
                pbar.update(1)

        if HAS_TQDM:
            pbar.close()

    # Return all results in order
    results = []
    for arch_name in architectures:
        for loss_category, _ in losses.items():
            combo_key = f"{arch_name}_{loss_category}"
            if combo_key in completed_results:
                results.append(completed_results[combo_key])

    return results


def select_top_k(
    results: List[ScreeningResult],
    k: int = 3,
    margin: float = 0.02,
) -> Tuple[List[str], List[str]]:
    """Select top-k architectures and losses with safety margin.

    Args:
        results: Screening results
        k: Number to select
        margin: Safety margin for including extras

    Returns:
        (top_architectures, top_losses)
    """
    # Aggregate by architecture
    arch_scores = {}
    for r in results:
        if r.r2 > float("-inf"):
            if r.architecture not in arch_scores:
                arch_scores[r.architecture] = []
            arch_scores[r.architecture].append(r.r2)

    arch_means = {a: np.mean(scores) for a, scores in arch_scores.items()}

    # Aggregate by loss
    loss_scores = {}
    for r in results:
        if r.r2 > float("-inf"):
            if r.loss not in loss_scores:
                loss_scores[r.loss] = []
            loss_scores[r.loss].append(r.r2)

    loss_means = {l: np.mean(scores) for l, scores in loss_scores.items()}

    # Select top-k with safety margin
    def select_with_margin(scores: Dict[str, float], k: int, margin: float) -> List[str]:
        sorted_items = sorted(scores.items(), key=lambda x: -x[1])
        selected = [x[0] for x in sorted_items[:k]]

        # Check if (k+1)th is within margin
        if len(sorted_items) > k:
            kth_score = sorted_items[k - 1][1]
            next_score = sorted_items[k][1]
            if kth_score - next_score < margin:
                selected.append(sorted_items[k][0])

        return selected

    top_archs = select_with_margin(arch_means, k, margin)
    top_losses = select_with_margin(loss_means, k, margin)

    return top_archs, top_losses


# =============================================================================
# Main Runner
# =============================================================================

def run_tier1(
    X: torch.Tensor,
    y: torch.Tensor,
    device: torch.device,
    architectures: Optional[List[str]] = None,
    losses: Optional[Dict[str, str]] = None,
    data_fraction: float = SCREEN_DATA_FRACTION,
    n_epochs: int = SCREEN_EPOCHS,
    seed: int = 42,
    register: bool = True,
    resume: bool = True,
) -> Tier1Result:
    """Run Tier 1: Quick Screening.

    Args:
        X: Input data [N, C, T]
        y: Target data [N, C, T]
        device: Device
        architectures: List of architecture names
        losses: Dict of loss_category -> loss_name
        data_fraction: Fraction of data to use
        n_epochs: Training epochs
        seed: Random seed
        register: Whether to register with ConfigRegistry
        resume: Whether to resume from checkpoint

    Returns:
        Tier1Result with top selections
    """
    architectures = architectures or ARCHITECTURES
    losses = losses or LOSS_CATEGORIES

    print(f"\nRunning {len(architectures)} architectures Ã— {len(losses)} losses = {len(architectures) * len(losses)} combinations")
    print(f"Data: {int(data_fraction * 100)}%, Epochs: {n_epochs}\n")

    # Run screening matrix
    results = run_screening_matrix(
        X=X,
        y=y,
        architectures=architectures,
        losses=losses,
        device=device,
        data_fraction=data_fraction,
        n_epochs=n_epochs,
        seed=seed,
        resume=resume,
    )

    # Select top performers
    top_archs, top_losses = select_top_k(results, k=3, margin=0.02)

    # Get best RÂ²
    valid_results = [r for r in results if r.r2 > float("-inf")]
    best_r2 = max(r.r2 for r in valid_results) if valid_results else 0.0

    # Get classical floor from registry
    registry = get_registry(ARTIFACTS_DIR)
    classical_floor = registry.classical_floor if registry.tier0 else 0.0

    tier1_result = Tier1Result(
        results=results,
        top_architectures=top_archs,
        top_losses=top_losses,
        best_r2=best_r2,
        classical_floor=classical_floor,
    )

    # Register
    if register:
        registry.register_tier1(tier1_result)

    # Clear checkpoint on successful completion
    _clear_checkpoint()

    return tier1_result


# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Tier 1: Quick Screening")
    parser.add_argument("--dry-run", action="store_true", help="Use minimal configuration")
    parser.add_argument("--epochs", type=int, default=SCREEN_EPOCHS, help="Training epochs")
    parser.add_argument("--data-fraction", type=float, default=SCREEN_DATA_FRACTION, help="Data fraction")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--device", type=str, default="cuda", help="Device")
    parser.add_argument("--output", type=str, default=None, help="Output JSON file")

    args = parser.parse_args()

    device = torch.device(args.device if torch.cuda.is_available() else "cpu")

    print("=" * 60)
    print("TIER 1: Quick Screening")
    print("=" * 60)
    print("Purpose: Eliminate inferior options before heavy compute")
    print("Method: 8 architectures Ã— 6 losses on 20% data")
    print()

    # Load data
    print("Loading data...")

    if args.dry_run:
        # Minimal data
        N = 100
        C = 32
        T = 500
        X = torch.randn(N, C, T)
        y = torch.randn(N, C, T)
        # Use only 2 architectures and 2 losses for dry-run
        architectures = ["linear", "cnn"]
        losses = {"huber": "huber", "spectral": "multi_scale_spectral"}
        n_epochs = 3
        data_fraction = 0.5
        print(f"  [DRY-RUN] Using synthetic data: {X.shape}")
    else:
        try:
            from data import prepare_data
            data = prepare_data()

            # Correct data loading: prepare_data returns flat dict with indices
            train_idx = data["train_idx"]
            val_idx = data["val_idx"]

            # ob = input (OB signals), pcx = target (PCx signals)
            ob = data["ob"]   # [n_samples, 32, time]
            pcx = data["pcx"]  # [n_samples, 32, time]

            # Combine train+val for training (test is held out for Tier 4)
            cv_idx = np.concatenate([train_idx, val_idx])

            # Subsample time steps to fit in GPU memory (5000 -> 1000)
            # This is valid: we're testing architecture patterns, not final performance
            time_subsample = 5  # Every 5th timestep: 5000 -> 1000
            X = torch.from_numpy(ob[cv_idx][:, :, ::time_subsample]).float()
            y = torch.from_numpy(pcx[cv_idx][:, :, ::time_subsample]).float()
            print(f"  Subsampled time: 5000 -> {X.shape[2]} (every {time_subsample}th step)")

            architectures = ARCHITECTURES
            losses = LOSS_CATEGORIES
            n_epochs = args.epochs
            data_fraction = args.data_fraction
            print(f"  Loaded real data: X={X.shape}, y={y.shape}")
            print(f"  Training samples: {len(cv_idx)} (train+val combined)")
        except Exception as e:
            import traceback
            traceback.print_exc()
            print(f"  ERROR: Could not load data ({e})")
            print(f"  Falling back to synthetic data for testing...")
            N = 200
            C = 32
            T = 1000
            X = torch.randn(N, C, T)
            y = torch.randn(N, C, T)
            architectures = ARCHITECTURES
            losses = LOSS_CATEGORIES
            n_epochs = args.epochs
            data_fraction = args.data_fraction

    # Run screening
    result = run_tier1(
        X=X,
        y=y,
        device=device,
        architectures=architectures,
        losses=losses,
        data_fraction=data_fraction,
        n_epochs=n_epochs,
        seed=args.seed,
    )

    # Summary
    print("\n" + "=" * 60)
    print("TIER 1 RESULTS: Screening Matrix")
    print("=" * 60)

    # Print matrix
    print("\nRÂ² Matrix:")
    print("-" * 60)

    # Get unique archs and losses
    archs = sorted(set(r.architecture for r in result.results))
    loss_cats = sorted(set(r.loss for r in result.results))

    # Header
    print(f"{'Architecture':15s}", end="")
    for loss in loss_cats:
        print(f" {loss:>10s}", end="")
    print()

    # Rows
    for arch in archs:
        print(f"{arch:15s}", end="")
        for loss in loss_cats:
            r = next((r for r in result.results if r.architecture == arch and r.loss == loss), None)
            if r and r.r2 > float("-inf"):
                print(f" {r.r2:10.4f}", end="")
            else:
                print(f" {'FAIL':>10s}", end="")
        print()

    print("\n" + "-" * 60)
    print(f"Top Architectures: {result.top_architectures}")
    print(f"Top Losses: {result.top_losses}")
    print(f"Best RÂ²: {result.best_r2:.4f}")

    # Check gate
    margin = result.best_r2 - result.classical_floor
    print(f"\nClassical Floor: {result.classical_floor:.4f}")
    print(f"Margin over classical: {margin:.4f}")
    if margin >= 0.05:
        print("GATE: PASSED (neural beats classical)")
    else:
        print("GATE: FAILED (neural doesn't beat classical by sufficient margin)")

    # Save
    output_file = args.output or (ARTIFACTS_DIR / f"tier1_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    output_file = Path(output_file)

    with open(output_file, "w") as f:
        json.dump(result.to_dict(), f, indent=2)

    print(f"\nResults saved to: {output_file}")

    return result


if __name__ == "__main__":
    main()
